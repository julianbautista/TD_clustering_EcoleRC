{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee2f777c",
   "metadata": {},
   "source": [
    "## Cosmological constraints with BAO\n",
    "I provide you with a series of power spectrum monopole measurements, at effective redshifts $z_{\\mathrm{zeff}}$ = 0.1 to 1.9.\n",
    "In this session, you will:\n",
    "\n",
    "1) measure the BAO isotropic parameter $\\alpha(z_{\\mathrm{eff}}) = \\left[D_{V}(z_{\\mathrm{eff}})/r_{\\mathrm{drag}}\\right] / \\left[D_{V}^{\\mathrm{fid}}(z_{\\mathrm{eff}})/r_{\\mathrm{drag}}^{\\mathrm{fid}}\\right]$ with each power spectrum\n",
    "\n",
    "2) with these measurements, you will constrain $\\Omega_{m}, \\Omega_{k}$ (hence detect dark energy!)\n",
    "\n",
    "3) provided a value of $r_{\\mathrm{drag}}$ (in $\\mathrm{Mpc}$), you will perform an \"inverse distance ladder\" analysis: fit all the $\\alpha(z_{\\mathrm{eff}})$ by varying $H_{0}$ and $\\Omega_{m}$\n",
    "\n",
    "4) those finding the correct $\\Omega_{m}$, $\\Omega_{k}$ and $H_{0}$ (I know the truth, you don't!) win!\n",
    "\n",
    "5) do you remember how to compute the correlation function? (last year's TD) What about the power spectrum? If any question, please ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c221ac",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3e59d",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "**If packages are already installed, skip this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b954b3-c755-4f38-b7d7-edc2f210fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Google Colab: I have already prepared the environment,\n",
    "# let's download it.\n",
    "# This may be faster than pip install below.\n",
    "#!python -m pip install gdown\n",
    "#!gdown https://drive.google.com/uc?id=1IKWH9vW5Zh910K7FZ4bbrNZaejvD9_Ru\n",
    "#!unzip -oq TD_clustering_venv.zip\n",
    "#import sys\n",
    "#sys.path.append('./TD_clustering_venv/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5526ef59-72a0-4368-8475-7942aea67b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively:\n",
    "!python -m pip install matplotlib cython emcee corner camb\n",
    "!python -m pip install git+https://github.com/cosmodesi/cosmoprimo#egg=cosmoprimo[camb,astropy]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b6fed",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Within the DESI collaboration we have put some effort into setting up some ~ easy to use Python packages for standard clustering analyses. Check them out here: https://github.com/cosmodesi.\n",
    "You may e.g. be interested by:\n",
    "- [cosmoprimo](https://github.com/cosmodesi/cosmoprimo): primordial cosmology (class, camb, isitgr, fftlog, interpolator, BAO filtering)\n",
    "- [pycorr](https://github.com/cosmodesi/pycorr): correlation function estimation\n",
    "- [pypower](https://github.com/cosmodesi/pypower): power spectrum (and window function) estimation\n",
    "- [pyrecon](https://github.com/cosmodesi/pyrecon): standard BAO reconstruction\n",
    "- [mockfactory](https://github.com/cosmodesi/mockfactory): tools to be build fast mocks\n",
    "- [desilike](https://github.com/cosmodesi/desilike): DESI likelihoods, fits of 2-pt statistics, Fisher, bindings with cosmological samplers (Cobaya, CosmoSIS, MontePython)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea428e",
   "metadata": {},
   "source": [
    "#### Measurements\n",
    "I generated fake (Gaussian-distributed) power spectrum measurements, in a fake cosmology; these are in the \"sims\" folder of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/adematti/TD_clustering.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'TD_clustering')  # add to PYTHONPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92de99",
   "metadata": {},
   "source": [
    "### A look at power spectrum measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfeea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from environment import Measurement, list_path_sim_measurement  # specify paths\n",
    "\n",
    "list_path = list_path_sim_measurement()\n",
    "list_data = []\n",
    "\n",
    "for path in list_path:\n",
    "    m = Measurement.load(path)\n",
    "    print('At effective redshift {:.2f} first k are {}, pk0 {}'.format(m.attrs['zeff'], m.x[:2], m.y[0,:2]))\n",
    "    plt.plot(m.x, m.x * m.y[0], label='$z = {:.2f}$'.format(m.attrs['zeff']))\n",
    "    list_data.append(m)\n",
    "\n",
    "plt.xlabel('$k$ [$h/\\mathrm{Mpc}$]')\n",
    "plt.ylabel('$k P(k)$ [$(\\mathrm{Mpc}/h)^{2}$]')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# m is an instance of Measurement\n",
    "# k (wavenumber) array is m.x\n",
    "# pk (power spectrum) is m.y\n",
    "# m.y is of shape (1, len(m.x)): the first Legendre multipole (monopole).\n",
    "print('Attributes: {}'.format(m.attrs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de719ee",
   "metadata": {},
   "source": [
    "### Isotropic BAO power spectrum model\n",
    "\n",
    "The principle of BAO fits is to measure *only* the position of the BAO, marginalizing over the shape of the power spectrum, for this measurement to be robust to distortions due to observational systematics.\n",
    "For this we decompose the fiducial power spectrum into *wiggle* and *no-wiggle* parts. We will allow the *wiggle* part to move, keeping the *no-wiggle* part fixed, adding $k$-dependent nuisance terms (*broadband terms*) to adjust the shape of the data power spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First generate a linear power spectrum in $(\\mathrm{Mpc}/h)^{3}$ units; z = 0 is enough.\n",
    "# (Indeed, linear growth factor D^{2}(z) is degenerate with B^2 in the model below)\n",
    "# Fiducial cosmological parameters are:\n",
    "# dict(Omega_m=0.31, Omega_b=0.022/0.676**2, h=0.676, sigma8=0.8, n_s=0.97, m_ncdm=0.06)\n",
    "# Several options:\n",
    "# 1) use cosmoprimo (engine='class' or engine='camb'), see https://github.com/cosmodesi/cosmoprimo/blob/main/nb/examples.ipynb\n",
    "#from cosmoprimo import Cosmology\n",
    "#cosmo_fid = Cosmology(Omega_m=0.31, omega_b=0.022, h=0.676, sigma8=0.8, n_s=0.97, m_ncdm=[0.06], engine='camb')\n",
    "#klin = np.geomspace(1e-4, 5., 2000)\n",
    "#pk_interpolator = cosmo_fid.get_fourier().pk_interpolator().to_1d(z=0.)\n",
    "#pklin = pk_interpolator(klin)\n",
    "#np.savetxt('pklin.txt', np.column_stack([klin, pklin]))\n",
    "# 2) use classy, camb...\n",
    "# 3) load precomputed linear power spectrum:\n",
    "#klin, pklin = np.loadtxt('pklin.txt', unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069a9ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then you need a smooth power spectrum, matching the linear power spectrum but without BAO wiggles.\n",
    "# Several options:\n",
    "# 1) use cosmoprimo\n",
    "#from cosmoprimo import PowerSpectrumInterpolator1D\n",
    "#pk_interpolator = PowerSpectrumInterpolator1D(klin, pklin, extrap_kmin=1e-4, extrap_kmax=1e2)\n",
    "#from cosmoprimo import PowerSpectrumBAOFilter\n",
    "#pknow = PowerSpectrumBAOFilter(pk_interpolator, engine='wallish2018').smooth_pk_interpolator()(klin)\n",
    "# 2) fit the linear power spectrum in log/log with a polynomial (Hinton2017); not perfect but good enough for this notebook\n",
    "#logk = np.log(klin)\n",
    "#logpk = np.log(pklin)\n",
    "#maxk = logk[pklin.argmax()]\n",
    "#gauss = np.exp(-0.5 * ((logk - maxk) / 1.)**2)\n",
    "#w = np.ones_like(klin) - 0.5 * gauss\n",
    "#series = np.polynomial.polynomial.Polynomial.fit(logk, logpk, deg=13 ,w=w)\n",
    "#pknow = np.exp(series(logk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2248fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot wiggles = linear power spectrum / no-wiggle power spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d01e4d",
   "metadata": {},
   "source": [
    "Model equation is (https://arxiv.org/abs/1705.06373, but many other formulations exist):\n",
    "$P(k,\\alpha) = B^{2} [P_{\\mathrm{nw}}(k) + \\sum_{i=0}^{2} A_{i} k^{i}] \\lbrace 1 + [\\mathcal{O}(k/\\alpha) - 1] e^{- \\frac{1}{2} \\Sigma_{\\mathrm{nl}}^{2}k^{2}} \\rbrace$  \n",
    "$P_{\\mathrm{nw}}$ is the no-wiggle power spectrum.  \n",
    "$\\mathcal{O}(k) = P_{\\mathrm{lin}}(k)/P_{\\mathrm{nw}}(k)$ are the wiggles. \n",
    "Free parameters are: $\\alpha, B, A_{i}$, parameter of cosmological interest is $\\alpha$ (which shifts BAO wiggles).\n",
    "Choose for the non-linear damping of BAO wiggles $\\Sigma_{\\mathrm{nl}} = 6.7\\;\\mathrm{Mpc/h}$.\n",
    "See https://arxiv.org/abs/2404.03000 and https://arxiv.org/abs/2402.14070 for the BAO theory used in DESI, including a new broadband parameterization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54290a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmanl = 6.7\n",
    "\n",
    "def pk_iso(k, alpha, B, A0, A1, A2):\n",
    "    \"\"\"Code here the BAO power spectrum model.\"\"\"\n",
    "    ipknow = np.interp(k, klin, pknow)\n",
    "    poly = A0 + A1 * k + A2 * k**2\n",
    "    damped_wiggles = \"code wiggles shifted by alpha here\"\n",
    "    return B**2 * (ipknow + poly) * damped_wiggles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcb661",
   "metadata": {},
   "source": [
    "### Model fitting\n",
    "Repeat, for each redshift z:  \n",
    "- build $\\chi^{2} = (\\mathrm{data} - \\mathrm{model})^T C^{-1} (\\mathrm{data} - \\mathrm{model})$\n",
    "- minimize chi2 for example with scipy.optimize.minimize, and check the fit looks good by plotting data and best fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "nbb = 3\n",
    "init = [1., 2.] + [0.] * nbb  # alpha, B, broadbands\n",
    "bounds = [(0.88, 1.12), (1., 5.)] + [(-5e4, 5e4)] * nbb\n",
    "\n",
    "for m in list_data:\n",
    "    covariance = m.cov\n",
    "    invcovariance = np.linalg.inv(covariance)\n",
    "    k = m.x\n",
    "    data = m.y[0]\n",
    "    \n",
    "    def chi2(args):\n",
    "        \"\"\"Write chi2, args is alpha, B, A0, A1, A2.\"\"\"\n",
    "        model = pk_iso(k, *args)  # return model\n",
    "        diff = data - model\n",
    "        toret = \"write chi2 here\"\n",
    "        return toret\n",
    "\n",
    "    \"\"\"Check reduced chi2, make plot of data, model, error bars.\"\"\"\n",
    "    #result = optimize.minimize(chi2, init, method='SLSQP', bounds=bounds)\n",
    "    #args = result.x  # best fit chi2\n",
    "    #minchi2 = \"chi2 at best fit\"\n",
    "    #msg = 'Reduced chi2 is {:.4f}/({:d} - {:d}) = {:.4f}'.format(minchi2, len(k), len(args), minchi2 / (len(k) - len(args)))\n",
    "    #print(msg)\n",
    "    \"\"\"Make plot of data, model, error bars.\"\"\"\n",
    "    #kplt = klin[(klin > k[0]) & (klin < k[-1])]\n",
    "    #plt.errorbar(k, k * data, k * np.diag(covariance) ** 0.5, linestyle='none', color='C0')\n",
    "    #plt.plot(kplt, kplt * pk_iso(kplt, *args), label='model', color='C0')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbe0d4",
   "metadata": {},
   "source": [
    "We want the PDF of $\\alpha$. Let's define the posterior and sample it with e.g. https://github.com/dfm/emcee. If you are not familiar with Bayesian inference, you can take a quick look at the corresponding notebook.\n",
    "$\\alpha$ posterior is well approximated by a Gaussian, so let's just take its mean and covariance for the downstream cosmological inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56979395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee   # pip install --user emcee\n",
    "import corner  # pip install --user corner\n",
    "\n",
    "nsteps = 4000\n",
    "ndim = len(bounds)\n",
    "nwalkers = 2 * ndim\n",
    "list_alpha_mean, list_alpha_covariance = [], []\n",
    "rng = np.random.RandomState(seed=42)\n",
    "\n",
    "for m in list_data:  # loop over all z's\n",
    "\n",
    "    covariance = m.cov\n",
    "    invcovariance = np.linalg.inv(covariance)\n",
    "    k = m.x\n",
    "    data = m.y[0]\n",
    "    \n",
    "    def chi2(args):\n",
    "        \"\"\"Write chi2, args is alpha, B, A0, A1, A2.\"\"\"\n",
    "    \n",
    "    def logprior(args):\n",
    "        \"\"\"\n",
    "        Write flat uniform prior between bounds, args is alpha, B, A0, A1, A2.\n",
    "        WARNING: This is the *log*-prior, so...\n",
    "        0 (or constant) if bounds[i][0] < args[i] < bounds[i][1]\n",
    "        -inf else.\n",
    "        \"\"\"\n",
    "    \n",
    "    # Write posterior here (take flat priors between bounds on all parameters)\n",
    "    def logposterior(args):\n",
    "        \"\"\"\n",
    "        Write log-posterior, args is alpha, B, A0, A1, A2.\n",
    "        WARNING: This is the *log*-posterior.\n",
    "        \"\"\"\n",
    "    \n",
    "    #sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
    "    #start = [[rng.normal(v, (b[1] - b[0]) / 10.) for v, b in zip(init, bounds)] for i in range(nwalkers)]\n",
    "    #sampler.run_mcmc(start, nsteps)\n",
    "    #samples = sampler.get_chain(flat=True)\n",
    "    # Look at samples, remove burnin\n",
    "    #samples = samples[len(samples) // 2:]\n",
    "    # Take mean (or median) and covariance of samples\n",
    "    #mean = np.mean(samples[:, 0])\n",
    "    #covariance = np.cov(samples[:, 0])\n",
    "    #list_alpha_mean.append(mean)\n",
    "    #list_alpha_covariance.append(covariance)\n",
    "    #labels = ['alpha', 'B'] + ['A{:d}'.format(i) for i in range(nbb)]\n",
    "    #fig = corner.corner(samples, labels=labels, quantiles=[0.16, 0.5, 0.84], show_titles=True, title_kwargs={'fontsize': 14})\n",
    "    #plt.show()\n",
    "    #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ecfc9",
   "metadata": {},
   "source": [
    "### Expansion history\n",
    "The measured $\\alpha$ can be modelled by $\\alpha_{\\mathrm{theory}}(z) = \\left[D_{V}(z_{\\mathrm{eff}})/r_{\\mathrm{drag}}\\right] / \\left[D_{V}^{\\mathrm{fid}}(z_{\\mathrm{eff}})/r_{\\mathrm{drag}}^{\\mathrm{fid}}\\right]$ (why?)  \n",
    "$D_{V}(z) = z^{1/3} D_{M}(z)^{2/3} D_{H}(z)^{1/3}$ with $D_{M}(z)$ comoving transverse distance, $D_{H}(z) = c/H(z)$ \"Hubble distance\". Factor $z^{1/3}$ is purely conventional.\n",
    "\n",
    "Code up $D_{M}(z)$, $D_{H}(z)$ as a function of $\\Omega_{m}$ and $\\Omega_{k}$ (faster than creating a new cosmology at each MCMC step).  \n",
    "Build the $\\chi^{2}$ of the previously measured $\\alpha$, using $\\alpha_{\\mathrm{theory}}$ as a model.\n",
    "Previously measured $\\alpha$ `list_alpha_mean` should be considered independent (zero cross-correlations); hence the covariance matrix is diagonal, with `list_alpha_covariance` as diagonal elements.\n",
    "This $\\chi^{2}$ depends on $r_{\\mathrm{drag}}$ (in $\\mathrm{Mpc}/h$), $\\Omega_{m}$ and $\\Omega_{k}$ if free curvature (entering $D_{M}$ and $D_{H}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18dd95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_z, list_dv_over_rs_fid = [], []\n",
    "\n",
    "for data in list_data:\n",
    "    z = data.attrs['zeff']\n",
    "    #list_z.append(z)\n",
    "    # Precompute DV_fid / rs_drag_fid\n",
    "    #dv = z**(1. / 3.) * ((1 + z) * cosmo_fid.angular_diameter_distance(z))**(2. / 3.) * (100. * cosmo_fid.efunc(z))**(-1. / 3.) /\n",
    "    #list_dv_over_rs_fid.append(dv)\n",
    "\n",
    "list_z = np.array(list_z)\n",
    "list_dv_over_rs_fid = np.array(list_dv_over_rs_fid)\n",
    "\n",
    "# Define Hubble parameter, comoving transverse distance, spherically-averaged distance DV\n",
    "# model, function of (rs_drag, Omega_m, Omega_k), is DVf / rs_drag / (DV_fid / rs_drag_fid) at each data z.\n",
    "# You can use cosmoprimo, but I recommend that you recode this up from the math, the inference will run faster.\n",
    "# If running on Google Colab, you may ask the AI! (but be careful ;))\n",
    "\n",
    "def spherically_averaged_distance(z, Omega_m=0.3, Omega_k=0.):\n",
    "    # return spherically averaged distance at z for given Omega_m and Omega_k\n",
    "\n",
    "def list_alpha_model(rs, *args):\n",
    "    return np.array([spherically_averaged_distance(z, *args) for z in list_z]) / rs / list_dv_over_rs_fid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1021d",
   "metadata": {},
   "source": [
    "Use a very wide prior on $r_{\\mathrm{drag}}$ as we don't want to make any assumption about the primordial Universe: we only assume constant comoving $r_{\\mathrm{drag}}$ (\"standard ruler\") to constrain expansion history.\n",
    "Sample the posterior and draw the contours $\\Omega_{m} - \\Omega_{k}$. See? we detect dark energy! (and zero curvature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c07a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2(args):\n",
    "    \"\"\"Write chi2, assuming independent measurements at each redshift.\"\"\"\n",
    "    toret = 0.\n",
    "    for data_alpha, cov, model_alpha in zip(list_alpha_mean, list_alpha_covariance, list_alpha_model(*args)):\n",
    "        # Fill in\n",
    "        #toret += ...\n",
    "    return toret\n",
    "\n",
    "init = [100., 0.3, 0.]  # rs_drag [Mpc/h], Omega_m, Omega_k\n",
    "bounds = [(20, 1000), (0.01, 0.9), (-0.8, 0.8)]\n",
    "\n",
    "def logprior(args):\n",
    "    \"\"\"Write flat uniform prior between bounds.\"\"\"\n",
    "\n",
    "def logposterior(args):\n",
    "    \"\"\"Write log-posterior.\"\"\"\n",
    "\n",
    "#ndim = len(init)\n",
    "#nwalkers = 4 * ndim\n",
    "#nsteps = 2000\n",
    "#sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
    "#start = [[rng.normal(v, (b[1] - b[0]) / 40.) for v, b in zip(init, bounds)] for i in range(nwalkers)]\n",
    "#sampler.run_mcmc(start, nsteps)\n",
    "#samples = sampler.get_chain(flat=True)\n",
    "# Remove burnin and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a315d",
   "metadata": {},
   "source": [
    "### Inverse distance ladder\n",
    "Do the same, but for $r_{\\mathrm{drag}}$ use a Gaussian prior $r_{\\mathrm{drag}} = 149.47 \\pm 0.48 \\; \\mathrm{Mpc}$.  \n",
    "Assume zero curvature for this exercise (though you can let it free in general, in addition to different dark energy models --- all those are constrained by BAO).\n",
    "Sample the posterior and draw the $\\Omega_{m} - H_{0}$ contours. See? given $r_{\\mathrm{drag}}$ we constrain $H_{0}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebe843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need to redefine the model as a function of rs_drag in Mpc, H0 and Omega_m,\n",
    "# and the Gaussian prior on rs_drag\n",
    "\n",
    "def list_alpha_model(rs, H0, *args):\n",
    "    return np.array([spherically_averaged_distance(z, *args) for z in list_z]) / (H0 / 100.) / rs / list_dv_over_rs_fid\n",
    "\n",
    "init = [149.47, 70, 0.3]  # rs_drag, H0, Omega_m \n",
    "bounds = [(100, 200), (50, 100), (0.1, 0.5)]\n",
    "\n",
    "def logprior(args):\n",
    "    \"\"\"Write flat uniform (log-)prior between bounds, and Gaussian prior on rs_drag.\"\"\"\n",
    "\n",
    "#ndim = len(init)\n",
    "#nwalkers = 4 * ndim\n",
    "#nsteps = 3000\n",
    "#sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
    "#start = [[rng.normal(v, (b[1] - b[0]) / 10.) for v, b in zip(init, bounds)] for i in range(nwalkers)]\n",
    "#sampler.run_mcmc(start, nsteps)\n",
    "#samples = sampler.get_chain(flat=True)\n",
    "# Remove burnin and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c4af19",
   "metadata": {},
   "source": [
    "### BBN prior\n",
    "CMB constraints provide a measurement of $r_{\\mathrm{drag}}$ (through $\\omega_{cdm}$ and $\\omega_{b}$),\n",
    "but we only need a prior on $\\omega_{b} = \\Omega_{b} h^{2}$ since $\\Omega_{m}$ is constrained by BAO mesurements.  $\\omega_{b} = \\Omega_{b} h^{2}$ is constrained by measurements of deuterium abundance as a result of BBN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3d528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute rs_drag, either:\n",
    "# 1) use cosmoprimo (engine='class')\n",
    "#def rs_drag(omega_b, omega_m):\n",
    "#    return cosmo_fid.clone(omega_b=omega_b, omega_m=omega_m, engine='class').rs_drag / cosmo_fid.h\n",
    "# 2) use cosmoprimo (engine='eisentein_hu') --- faster\n",
    "#def rs_drag(omega_b, omega_m):\n",
    "#    return cosmo_fid.clone(omega_b=omega_b, omega_m=omega_m, engine='eisenstein_hu').rs_drag / cosmo_fid.h\n",
    "# 3) implement rs_drag yourself, following https://arxiv.org/abs/astro-ph/9709112, eq. 4 - 6.\n",
    "# For $z_{\\mathrm{drag}}$, rather use https://arxiv.org/abs/astro-ph/9510117, eq. E1 (found better match to Boltzmann code).\n",
    "# Fix $T_\\mathrm{cmb} = 2.7255$ (Cobe-FIRAS).\n",
    "#def rs_drag(omega_b, omega_m):\n",
    "#    \"\"\"Write approximation for rs_drag as a function of omega_b, omega_m\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d4e51",
   "metadata": {},
   "source": [
    "Redo the previous sampling, this time varying $H_{0}$, $\\Omega_{m}$ and $\\omega_{b}$ with the BBN prior: $\\omega_{b} = 0.02235 \\pm 0.00037$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ca862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need to redefine the model as a function of omega_b, H0 and Omega_m, and the Gaussian prior on omega_b\n",
    "\n",
    "def list_alpha_model(omega_b, H0, Omega_m):\n",
    "    h = H0 / 100.\n",
    "    rs = rs_drag(omega_b, Omega_m * h**2)\n",
    "    return np.array([spherically_averaged_distance(z, Omega_m) for z in list_z]) / h / rs / list_dv_over_rs_fid\n",
    "\n",
    "# For omega_b, H0, Omega_m\n",
    "init = [0.02235, 70, 0.3]  # omega_b, H0, Omega_m\n",
    "bounds = [(0.01, 0.03), (50, 100), (0.1, 0.5)]\n",
    "\n",
    "def logprior(args):\n",
    "    \"\"\"Write flat uniform (log-)prior between bounds, and Gaussian prior on omega_b.\"\"\"\n",
    "\n",
    "#ndim = len(init)\n",
    "#nwalkers = 4 * ndim\n",
    "#nsteps = 3000\n",
    "#sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior)\n",
    "#start = [[rng.normal(v, (b[1] - b[0]) / 10.) for v, b in zip(init, bounds)] for i in range(nwalkers)]\n",
    "#sampler.run_mcmc(start, nsteps)\n",
    "#samples = sampler.get_chain(flat=True)\n",
    "# Remove burnin and plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5493f13a",
   "metadata": {},
   "source": [
    "## Take-home messages\n",
    "- The BAO feature is a standard ruler: measuring the position of the BAO peak = measuring a fixed comoving distance (sound horizon at the drag epoch) at a given redshift = constraing the Universe's expansion\n",
    "- assuming standard ruler only (of unspecified length), we can constrain the 'derivatives of the expansion', i.e. the matter / dark energy density\n",
    "- with an additional prior on $r_{\\mathrm{drag}}$ (or $\\omega_{b}$), we can measure $H_{0}$ = 'inverse distance ladder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2bbdf",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "- see how $\\Omega_{m} - H_{0}$ contour rotates with redshift\n",
    "- perform fits to real eBOSS data (see eBOSS_LRGpCMASS.ipynb for power spectrum measurements): isotropic, anisotropic BAO fits, RSD fits, ...\n",
    "- why are you still here? go to the beach..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmodesi-main",
   "language": "python",
   "name": "cosmodesi-main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
